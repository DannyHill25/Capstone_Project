{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e11527-9846-4f72-8e5a-e2fcbd86c8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "from pprint import pprint # Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel # spaCy for preprocessing\n",
    "import spacy # Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4d94f-8928-4217-985d-4b96c005dba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spam = pd.read_csv('C:\\\\Users\\\\danih\\\\OneDrive\\\\Desktop\\\\Digital Futures Academy\\\\Capstone\\\\spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313ecb5-4fc8-49f0-8762-a0d1b27bd531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = spam['v2']\n",
    "y = spam['v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a23345-3305-4912-bc25-60b86f3fd74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b57fcd-60f0-4f11-ae3d-48fba3c133d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186a041-e661-40a7-9650-ef92c7b9a6d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spam_filter = svm.SVC()\n",
    "spam_filter.fit(features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4693462-0e6a-46e0-94df-2fb5468dbbb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_test = cv.transform(X_test)\n",
    "print(\"Accuracy: {}\".format(spam_filter.score(features_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f99be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locations = ['Buxton', 'Bollington', 'Biddulph', 'Leek', 'Cheadle']\n",
    "base_url = 'https://www.{}hearingcentre.co.uk/wp-json/wp/v2/submissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f332dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_json_data(url):\n",
    "    \"\"\"Fetch JSON data from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ba634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_location_data(locations, base_url):\n",
    "    \"\"\"\n",
    "    Generate a dictionary with location names as keys and JSON data as entries.\n",
    "    \n",
    "    Argument:\n",
    "    locations - List of location names.\n",
    "    base_url - Base URL for fetching JSON data, with '{}' placeholder for location name.\n",
    "    \n",
    "    Returns:\n",
    "    dict - Dictionary with location names as keys and JSON data as entries.\n",
    "    \"\"\"\n",
    "    hearing_centre_data = {}\n",
    "    \n",
    "    for location in locations:\n",
    "        # Replace placeholder with the actual location name\n",
    "        url = base_url.replace('{}', location.lower())\n",
    "        # Request permission\n",
    "        response = requests.get(url, verify=False)\n",
    "        # Store the data\n",
    "        data = response.json()\n",
    "        # Add to the dictionary\n",
    "        hearing_centre_data[location] = data\n",
    "    \n",
    "    return hearing_centre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb293396",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hearing_centre_data = generate_location_data(locations, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c81708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hearing_centre_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0ca59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_messages(location_data):\n",
    "    \"\"\"\n",
    "    Process the JSON data to extract messages and clean them.\n",
    "    \n",
    "    Argument:\n",
    "    location_data - Dictionary with location names as keys and JSON data as entries.\n",
    "    \n",
    "    Returns:\n",
    "    df - DataFrame containing the post date, post content message, email, and location.\n",
    "    \"\"\"\n",
    "    messages_data = []\n",
    "    email_map = {}\n",
    "    email_counter = 1\n",
    "    \n",
    "    for location, data in location_data.items():\n",
    "        # Loop through each item in the JSON data\n",
    "        for item in data:\n",
    "            # Check if 'post_content' key exists and is a string\n",
    "            if 'post_content' in item and isinstance(item['post_content'], str):\n",
    "                post_content = item['post_content'].split('\\n')\n",
    "                \n",
    "                # Extract the email \n",
    "                email = post_content[1] if len(post_content) > 1 else 'N/A'\n",
    "                \n",
    "                # Anonymise the email\n",
    "                if email not in email_map:\n",
    "                    email_map[email] = f'anonymised_email_{email_counter}'\n",
    "                    email_counter += 1\n",
    "                anonymized_email = email_map[email]\n",
    "                \n",
    "                # Initialize an empty message\n",
    "                message = []\n",
    "                \n",
    "                # Loop through the post_content to capture the message\n",
    "                for i in range(3, len(post_content)):\n",
    "                    if post_content[i].isdigit():\n",
    "                        break\n",
    "                    message.append(post_content[i])\n",
    "                \n",
    "                # Join the message parts and clean them\n",
    "                message_content = '\\n'.join(message).replace('\\r', '').replace('\\n', ' ').strip()\n",
    "                \n",
    "                # Extract and convert post date if it exists\n",
    "                # Use 'N/A' if post_date is not available\n",
    "                # post_date = pd.to_datetime(item.get('post_date', 'N/A')).date()\n",
    "                \n",
    "                # Extract and convert post date if it exists\n",
    "                post_date_str = item.get('post_date', None)\n",
    "                if post_date_str:\n",
    "                    try:\n",
    "                        post_date = pd.to_datetime(post_date_str)\n",
    "                    except Exception as e:\n",
    "                        post_date = None\n",
    "                else:\n",
    "                    post_date = None\n",
    "                \n",
    "                # Append the extracted data to the messages_data list\n",
    "                messages_data.append({\n",
    "                    'post_date': post_date,\n",
    "                    'post_content': message_content,\n",
    "                    'email': anonymized_email,\n",
    "                    'location': location\n",
    "                })\n",
    "    \n",
    "    # Create a DataFrame from the messages_data list\n",
    "    df = pd.DataFrame(messages_data)\n",
    "    \n",
    "    # Normalize 'post_date' to display date as 'YYYY-MM-DD'\n",
    "        # Timestamp is converted to '00:00:00', hence it's not displayed\n",
    "    df['post_date'] = df['post_date'].dt.normalize()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f976dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = process_messages(hearing_centre_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7257bf1-d646-4292-b349-14d471bcb309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acc3dc-00a5-4177-9fe1-89333a28d903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f5d3a-8e35-42d2-99f6-c296361d4fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(df, cv, spam_filter):\n",
    "    \"\"\"\n",
    "    Clean the DataFrame by removing duplicate entries and filtering out spam.\n",
    "\n",
    "    Argument:\n",
    "    df - The DataFrame to clean, with 'post_content' and 'email' columns.\n",
    "    cv - The CountVectorizer used to transform the text data.\n",
    "    spam_filter - The trained SVM model to predict spam.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Remove duplicates based on 'post_content' and 'email' columns\n",
    "    df = df.drop_duplicates(subset=['post_content', 'email'], keep='first')\n",
    "    \n",
    "    # Remove messages that contain links (http or https)\n",
    "    df = df[~df['post_content'].str.contains(r'http://|https://',\n",
    "                                             case=False,\n",
    "                                             na=False)]\n",
    "\n",
    "    # Transform the 'post_content' using the CountVectorizer\n",
    "    post_content_features = cv.transform(df['post_content'])\n",
    "\n",
    "    # Predict spam using the spam_filter model\n",
    "    spam_predictions = spam_filter.predict(post_content_features)\n",
    "\n",
    "    # Filter out entries labeled as spam\n",
    "    df = df[spam_predictions == 'ham']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973b19b-d109-43a3-ae1b-4c6d635f6569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean = clean_data(df, cv, spam_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765f950-7760-44a0-ac47-3e8114eb278c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40589778-ef7e-4902-b784-f28b272807b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence),\n",
    "                                             deacc = True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96b009-d0da-44a0-b327-40f3b1738631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(df_clean['post_content']))\n",
    "df_token = df_clean.copy()\n",
    "df_token['cleaned_post_content'] = [' '.join(words) for words in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508367fb-9753-4039-93e7-fb7a55a7cb42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words,\n",
    "                               min_count=5,\n",
    "                               threshold=100) \n",
    "trigram = gensim.models.Phrases(bigram[data_words],\n",
    "                                threshold=100) \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcf1da-a79e-4ba0-a12c-cb8274ec90e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e461f0-e421-4ceb-bc2e-5e77df36da99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm',\n",
    "                 disable = ['parser',\n",
    "                            'ner'])\n",
    "\n",
    "# Lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams,\n",
    "                                allowed_postags = ['NOUN',\n",
    "                                                   'ADJ',\n",
    "                                                   'VERB',\n",
    "                                                   'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40bb5f-c93a-46a8-9ebb-bacf4262e54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dictionary \n",
    "id2word = corpora.Dictionary(data_lemmatized)  \n",
    "# Create Corpus \n",
    "texts = data_lemmatized  \n",
    "# Term Document Frequency \n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bc73d-f66f-4935-89a7-a0090c329cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Topic Model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                           id2word = id2word,\n",
    "                                           num_topics = 30, \n",
    "                                           random_state = 25,\n",
    "                                           update_every = 1,\n",
    "                                           chunksize = 100,\n",
    "                                           passes = 10,\n",
    "                                           alpha = 'auto',\n",
    "                                           per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38673c60-f362-40c2-9775-dcb370e4cba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model = lda_model,\n",
    "                                     texts = data_lemmatized,\n",
    "                                     dictionary = id2word,\n",
    "                                     coherence = 'c_v'\n",
    "                                    )\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c5473-a5be-465a-80b8-ad8381dcdfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "#pyLDAvis.display(vis, template_type='notebook')\n",
    "pyLDAvis.save_html(vis, 'hearingcentretopics.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
